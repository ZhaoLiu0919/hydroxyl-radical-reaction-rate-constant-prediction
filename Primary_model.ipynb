{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c719b81",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "import re\n",
    "import openpyxl as xl\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Required RDKit modules\n",
    "import rdkit as rd\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit import RDConfig\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import Chem\n",
    "\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "\n",
    "# modeling\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, GridSearchCV, cross_val_score, train_test_split, validation_curve\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.metrics import classification_report, mean_squared_error, make_scorer, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "from rdkit.Chem.rdMolDescriptors import GetMACCSKeysFingerprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#from skopt import BayesSearchCV\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f7768",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predict_report(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = y_pred.reshape(-1)  # To make sure the format is array([1, 2, 3..]).\n",
    "    MAE = mean_absolute_error(y, y_pred)\n",
    "    RMSE = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    R2 = r2_score(y, y_pred)\n",
    "    print(f'MAE: {MAE}')\n",
    "    print(f'RMSE: {RMSE}')\n",
    "    print(f'R2: {R2}')\n",
    "\n",
    "    # Data scatter of predicted values\n",
    "    plt.figure();plt.clf()\n",
    "    plt.scatter(y, y_pred, marker='.', color='blue')\n",
    "    plt.xlabel(\"True value\")\n",
    "    plt.ylabel(\"Predicted value\")\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.show()\n",
    "\n",
    "    dict_test = {'MAE': MAE, 'RMSE': RMSE, 'R2': R2}\n",
    "    return dict_test\n",
    "\n",
    "def generate_train_report(model, X_train, y_train):\n",
    "    print(\"-------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Train report for model {model}:\")\n",
    "    return generate_predict_report(model, X_train, y_train)\n",
    "\n",
    "def generate_val_report(model, X_val, y_val):\n",
    "    print(\"-------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Validation report for model {model}:\")\n",
    "    return generate_predict_report(model, X_val, y_val)\n",
    "    \n",
    "def generate_test_report(model, X_test, y_test):\n",
    "    print(\"-------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Test report for model {model}:\")\n",
    "    return generate_predict_report(model, X_test, y_test)\n",
    "\n",
    "\n",
    "def data_split(df, X_total, y_total, random_state, n_clusters):\n",
    "    # Get the MACCS fingerprints\n",
    "    fingerprints = [MACCSkeys.GenMACCSKeys(Chem.MolFromSmiles(smi)) for smi in df['SMILES']]\n",
    "    \n",
    "    # Convert fingerprints to a numpy array\n",
    "    fp_matrix = np.array([list(fp) for fp in fingerprints])\n",
    "    \n",
    "    # Use KMeans clustering on the fingerprints\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=4).fit(fp_matrix)\n",
    "    df['cluster'] = kmeans.labels_\n",
    "\n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_val = []\n",
    "    y_test = []\n",
    "\n",
    "    # For each cluster, do a train-validation-test split\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = df[df['cluster'] == i].index\n",
    "        X_remain, X_test_cluster, y_remain, y_test_cluster = train_test_split(X_total[cluster_indices], y_total[cluster_indices], test_size=0.1, random_state=random_state)\n",
    "        X_train_cluster, X_val_cluster, y_train_cluster, y_val_cluster = train_test_split(X_remain, y_remain, test_size=0.1, random_state=random_state)\n",
    "\n",
    "        X_train.extend(X_train_cluster)\n",
    "        X_val.extend(X_val_cluster)\n",
    "        X_test.extend(X_test_cluster)\n",
    "        y_train.extend(y_train_cluster)\n",
    "        y_val.extend(y_val_cluster)\n",
    "        y_test.extend(y_test_cluster)\n",
    "        \n",
    "    return np.array(X_train), np.array(X_val), np.array(X_test), np.array(y_train), np.array(y_val), np.array(y_test)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "    val_metrics = calculate_metrics(y_val, y_val_pred)\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    return train_metrics, val_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b935a",
   "metadata": {},
   "source": [
    "# Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0171f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "df = pd.read_excel('./Datasets.xlsx', sheet_name='Primary(pH+T)',engine='openpyxl')\n",
    "df['mol'] = [AllChem.MolFromSmiles(smiles) for smiles in df['SMILES']]\n",
    "df['fp'] = [GetMACCSKeysFingerprint(mol) for mol in df['mol']]\n",
    "df_fp = [GetMACCSKeysFingerprint(mol) for mol in df['mol']]\n",
    "\n",
    "## Split FP to multiple columns so that they can be easily combined with others\n",
    "fp = pd.DataFrame(np.array(df_fp))\n",
    "\n",
    "## Combine with pH and T\n",
    "df_new = pd.concat([fp, df['pH'], df['T']], axis=1)\n",
    "display(df_new)\n",
    "print(df_new.shape)\n",
    "X_total = np.array(df_new)\n",
    "y_total = np.array(df['Log k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9903333",
   "metadata": {},
   "source": [
    "# Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_eval(n_estimators, max_depth, gamma, min_child_weight, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, max_delta_step, learning_rate, X_train_list, y_train_list, X_val_list, y_val_list, random_states, booster):\n",
    "    avg_score = 0\n",
    "    \n",
    "    for i, random_state in enumerate(random_states):\n",
    "        params = {\n",
    "            'n_estimators': int(n_estimators),\n",
    "            'max_depth': int(max_depth),\n",
    "            'gamma': gamma,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'colsample_bylevel': colsample_bylevel,\n",
    "            'colsample_bynode': colsample_bynode,\n",
    "            'reg_alpha': reg_alpha,\n",
    "            'reg_lambda': reg_lambda,\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'max_delta_step': int(max_delta_step),\n",
    "            'learning_rate': learning_rate,\n",
    "            'booster': booster,\n",
    "            'seed': int(random_state)\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        \n",
    "        model.fit(X_train_list[i], y_train_list[i])\n",
    "        \n",
    "        predictions = model.predict(X_val_list[i])\n",
    "        \n",
    "        # Taking the square root of the mean squared error to get the root mean squared error (RMSE)\n",
    "        score = np.sqrt(mean_squared_error(y_val_list[i], predictions))\n",
    "        avg_score += score\n",
    "    \n",
    "    # Return the negative average root mean squared error\n",
    "    return -avg_score / len(random_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1af2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_bayesian(df, X_total, y_total, random_states, n_clusters, n_iter=100, init_points=5):\n",
    "    X_train_list = []\n",
    "    X_val_list = []\n",
    "    y_train_list = []\n",
    "    y_val_list = []\n",
    "    \n",
    "    # Assuming data_split is a function defined elsewhere that splits the data\n",
    "    for random_state in random_states:\n",
    "        X_train, X_val, _, y_train, y_val, _ = data_split(df, X_total, y_total, random_state, n_clusters)\n",
    "        \n",
    "        # Adding the second dataset to the training set\n",
    "        X_train = np.concatenate((X_train, X_total_1), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_total_1), axis=0)\n",
    "        \n",
    "        X_train_list.append(X_train)\n",
    "        X_val_list.append(X_val)\n",
    "        y_train_list.append(y_train) \n",
    "        y_val_list.append(y_val)\n",
    "    \n",
    "    def xgboost_optimization(n_estimators, max_depth, gamma, min_child_weight, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, max_delta_step, learning_rate, booster_index):\n",
    "        booster = ['gbtree', 'dart'][int(booster_index)]\n",
    "        return xgboost_eval(n_estimators, max_depth, gamma, min_child_weight, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, max_delta_step, learning_rate, X_train_list, y_train_list, X_val_list, y_val_list, random_states, booster)\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=xgboost_optimization,\n",
    "        pbounds={\n",
    "            'n_estimators': (1000, 1500),\n",
    "            'max_depth': (40, 50),\n",
    "            'gamma': (0.1, 0.4),\n",
    "            'min_child_weight': (1, 5),\n",
    "            'subsample': (0.8, 1.0),\n",
    "            'colsample_bytree': (0.5, 0.9),\n",
    "            'colsample_bylevel': (0.5, 0.7),\n",
    "            'colsample_bynode': (0.6, 0.8),\n",
    "            'reg_alpha': (0.1, 0.5),\n",
    "            'reg_lambda': (0.5, 1.5),\n",
    "            'scale_pos_weight': (1.0, 5.0),\n",
    "            'max_delta_step': (1, 3),\n",
    "            'learning_rate': (0.02, 0.05),\n",
    "            'booster_index': (0, 1)\n",
    "        },\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer.maximize(n_iter=n_iter, init_points=init_points)\n",
    "\n",
    "    best_params = optimizer.max['params']\n",
    "\n",
    "    # Convert the continuous booster_index to its corresponding category\n",
    "    best_params['booster'] = ['gbtree', 'dart'][int(best_params.pop('booster_index'))]\n",
    "\n",
    "    # Make sure some of the parameters are integers\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['max_delta_step'] = int(best_params['max_delta_step'])\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b805c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = [0, 1, 2, 3, 4]\n",
    "best_params = hyperparameter_tuning_bayesian(df, X_total, y_total, random_states, n_clusters=70)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7f961",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b78743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the best hyperparameters to a variable\n",
    "best_params = {'colsample_bylevel': 0.5362386927724025, 'colsample_bynode': 0.7846605864264999, 'colsample_bytree': 0.5869205526328731, 'gamma': 0.120232287455155, 'learning_rate': 0.04247055522398333, 'max_delta_step': 1, 'max_depth': 49, 'min_child_weight': 1.407905723403665, 'n_estimators': 1247, 'reg_alpha': 0.4174734818527416, 'reg_lambda': 0.9800779693705134, 'scale_pos_weight': 4.374540085845599, 'subsample': 0.9287718676241973, 'booster': 'gbtree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c05c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_summary = pd.DataFrame(columns=['Random State', 'MAE_train', 'RMSE_train', 'R2_train', \n",
    "                                       'MAE_val', 'RMSE_val', 'R2_val', 'MAE_test', 'RMSE_test', 'R2_test'])\n",
    "\n",
    "for i in range(5):\n",
    "    random_state = i\n",
    "    n_clusters = 70  # Or any other number depending on your dataset\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = data_split(df, X_total, y_total, random_state, n_clusters)\n",
    "    \n",
    "    # Add the second dataset into training set\n",
    "    X_train = np.concatenate((X_train), axis=0)\n",
    "    y_train = np.concatenate((y_train), axis=0)\n",
    "    \n",
    "    model_optimized = xgb.XGBRegressor(**best_params)\n",
    "    model_optimized.fit(X_train, y_train)\n",
    "    train_report = generate_train_report(model_optimized, X_train, y_train)\n",
    "    val_report = generate_val_report(model_optimized, X_val, y_val)\n",
    "    test_report = generate_test_report(model_optimized, X_test, y_test)\n",
    "\n",
    "    results = pd.Series([i] + list(train_report.values()) + list(val_report.values()) + list(test_report.values()), index=result_summary.columns)\n",
    "    result_summary = result_summary.append(results, ignore_index=True)\n",
    "\n",
    "print(result_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
